---
title: "Lab 9 - HPC"
output: 
  github_document:
    df_print: paged
  html_document: default
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = T, include = T, warning = F, message = F)
```

# Learning goals

In this lab, you are expected to practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

## Problem 1

Give yourself a few minutes to think about what you learned about parallelization. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

1. Image processing for large datasets - Processing large sets of images for tasks such as object detection, segmentation, or enhancement can be computationally intensive. Each image can be processed independently. Packages: parallel, RcppParallel.

2. Big data analysis using MapReduce -  Analyzing large-scale datasets, such as log files or user data, using MapReduce techniques. Tasks like filtering, aggregating, and summarizing data can be parallelized efficiently. Packages - Rhipe, RHadoop.

3. Monte Carlo simulations - Can simplify Monte Carlo simulation studies by automatically setting up loops to run over parameter grids and parellelizing the repitions. Packagges - MonteCarlo. 

## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1}
library(microbenchmark)

fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}

fun1alt <- function(n = 100, k = 4, lambda = 4) {
  x <- matrix(rpois(n * k, lambda), nrow = n, ncol = k)
  return(x)
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(),
  fun1alt()
)
```

How much faster?

fun1() takes about 135 microseconds on average and the alternate one takes about 14 microseconds. 


2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
  x[cbind(max.col(t(x)), 1:ncol(x))]
}

# Benchmarking
benchmark <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x)
)

benchmark
```

```{r}
library(ggplot2)

benchmark_df <- as.data.frame(benchmark)

ggplot(benchmark_df, aes(x = expr, y = time / 1e6, fill = expr)) +
  geom_boxplot() +
  labs(title = "Execution Time Comparison: fun2 vs. fun2alt",
       x = "Function",
       y = "Execution Time (milliseconds)") +
  theme_minimal() +
  scale_fill_manual(values = c("#FF9999", "#9999FF"))
```


## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun}
library(parallel)

my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
  
  ans <- lapply(seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  ans
}

```

1. Use the previous pseudocode, and make it work with `parallel`. Here is just an example for you to try:

```{r p3-test-boot}
my_boot_parallel <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  # STEP 1:
  cl <- makeCluster(ncpus)
  # STEP 2: GOES HERE
  clusterExport(cl, varlist = c("dat", "stat", "idx"), envir = environment())
  
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  ans <- parLapply(cl, seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: GOES HERE
  stopCluster(cl)
  
  ans
  
}

# Bootstrap of a linear regression model
my_stat <- function(dat) {
  fit <- lm(y ~ x, data = dat)
  coef(fit)
} 

# DATA SIM
set.seed(1)
n <- 500 
R <- 1e4
x <- cbind(rnorm(n)) 
y <- x*5 + rnorm(n)
dat <- data.frame(x = x, y = y)

# Check if we get something similar as lm
ans0 <- confint(lm(y ~ x, data=dat))

# Using non-parallel function
ans1 <- my_boot(dat, my_stat, R)
boot_ci <- apply(ans1, 2, quantile, probs = c(0.025, 0.975))

# Using parallel function
ncpus <- detectCores() - 1
ans2 <- my_boot_parallel(dat, my_stat, R, ncpus)
boot_ci_parallel <- apply(ans2, 2, quantile, probs = c(0.025, 0.975))

print("Confidence intervals from lm:")
print(ans0)

print("Bootstrap confidence intervals (single-core):")
print(boot_ci)

print("Bootstrap confidence intervals (parallel):")
print(boot_ci_parallel)
```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3}
benchmark <- microbenchmark::microbenchmark(
  my_boot(dat, my_stat, R),
  my_boot_parallel(dat, my_stat, R, ncpus),
  times = 5
)

print(benchmark)

boxplot(benchmark, main = "Execution Time Comparison", ylab = "Time (milliseconds)")
```

We can see from the results that the parallel function runs much faster than the non-parallel one. 

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("[full-path-to-your-Rmd-file.Rmd]")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


